# ThaiPy 2024
1. Groundbreaking ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) paper by the Google Brain team issued in 2017.
2. [The explanation](https://datascience.stackexchange.com/questions/75855/what-types-of-matrix-multiplication-are-used-in-machine-learning-when-are-they) of matrices multiplication in neural networks. The stackexchange discussion.
3. [TRL](https://github.com/huggingface/trl/tree/main) â€” the way to train transformers with the reinforcement learning technics.
4. [Explanation](https://www.reddit.com/r/StableDiffusion/comments/xm7ndc/comment/ipmvdyn/) of Stable Diffusion by one of the reddit users.
5. [GGML/GGUF file format vulnerabilities](https://www.databricks.com/blog/ggml-gguf-file-format-vulnerabilities) by Databricks.
6. [How to Implement Multi-Head Attention from Scratch in Tensorflow and Keras](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/).
